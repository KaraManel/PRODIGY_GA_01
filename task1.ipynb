{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8921808,"sourceType":"datasetVersion","datasetId":5366183}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-10T11:14:37.017669Z","iopub.execute_input":"2024-07-10T11:14:37.017966Z","iopub.status.idle":"2024-07-10T11:15:04.787649Z","shell.execute_reply.started":"2024-07-10T11:14:37.017942Z","shell.execute_reply":"2024-07-10T11:15:04.786599Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\n\n# Load and tokenize the dataset\ndataset = load_dataset('text', data_files={'train': '/kaggle/input/txt-dataset/input.txt'})\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n\n# Set the padding token\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n\n# Prepare data for language modeling\nblock_size = 128\n\ndef group_texts(examples):\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result['labels'] = result['input_ids'].copy()\n    return result\n\nlm_datasets = tokenized_dataset.map(group_texts, batched=True)\n\n# Data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, \n    mlm=False\n)\n\n# Load pre-trained model\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    save_steps=10_000,\n    save_total_limit=2,\n    logging_dir='./logs',\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets['train'],\n    data_collator=data_collator,\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:37:26.533741Z","iopub.execute_input":"2024-07-10T11:37:26.534502Z","iopub.status.idle":"2024-07-10T12:49:00.034244Z","shell.execute_reply.started":"2024-07-10T11:37:26.534467Z","shell.execute_reply":"2024-07-10T12:49:00.033160Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b1003624366415381e626ad0894a55d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db80778751a40a295091e9e2a1a53e8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15000/15000 1:10:44, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>4.694500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>4.395900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>4.264000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>4.233200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>4.195200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>4.141100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>4.093300</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>4.054700</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>4.071400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>4.034200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>3.700400</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>3.656300</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>3.698000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>3.710500</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>3.697000</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>3.638600</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>3.660800</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>3.686900</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>3.614100</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>3.638200</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>3.402800</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>3.398100</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>3.435000</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>3.434000</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>3.461400</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>3.381900</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>3.453900</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>3.409200</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>3.429200</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>3.363900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=15000, training_loss=3.76825634765625, metrics={'train_runtime': 4244.7288, 'train_samples_per_second': 28.27, 'train_steps_per_second': 3.534, 'total_flos': 7838760960000000.0, 'train_loss': 3.76825634765625, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load model and tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n# Define the prompt\nprompt = \"First Citizen: Before we proceed any further, hear me speak.\"\ninputs = tokenizer(prompt, return_tensors='pt')\n\n# Move inputs to the same device as the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\n# Generate text with repetition mitigation strategies\noutputs = model.generate(\n    inputs['input_ids'], \n    max_length=50, \n    num_return_sequences=1, \n    no_repeat_ngram_size=2,  \n    top_k=50,  \n    top_p=0.95, \n    temperature=0.7,  \n    repetition_penalty=2.0  \n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:43:12.055828Z","iopub.execute_input":"2024-07-10T13:43:12.056297Z","iopub.status.idle":"2024-07-10T13:43:13.226020Z","shell.execute_reply.started":"2024-07-10T13:43:12.056263Z","shell.execute_reply":"2024-07-10T13:43:13.224967Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"First Citizen: Before we proceed any further, hear me speak.\nI am the first citizen of this world to be born in a country where there is no law against slavery and that it has been abolished by our government for over two hundred years;\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained('./shakespeare-gpt2')\ntokenizer.save_pretrained('./shakespeare-gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:46:18.940183Z","iopub.execute_input":"2024-07-10T13:46:18.940637Z","iopub.status.idle":"2024-07-10T13:46:19.943859Z","shell.execute_reply.started":"2024-07-10T13:46:18.940602Z","shell.execute_reply":"2024-07-10T13:46:19.942603Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"('./shakespeare-gpt2/tokenizer_config.json',\n './shakespeare-gpt2/special_tokens_map.json',\n './shakespeare-gpt2/vocab.json',\n './shakespeare-gpt2/merges.txt',\n './shakespeare-gpt2/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2Tokenizer\n\nmodel = GPT2LMHeadModel.from_pretrained('./shakespeare-gpt2')\ntokenizer = GPT2Tokenizer.from_pretrained('./shakespeare-gpt2')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T13:46:23.329778Z","iopub.execute_input":"2024-07-10T13:46:23.330138Z","iopub.status.idle":"2024-07-10T13:46:23.624967Z","shell.execute_reply.started":"2024-07-10T13:46:23.330107Z","shell.execute_reply":"2024-07-10T13:46:23.623603Z"},"trusted":true},"execution_count":32,"outputs":[]}]}